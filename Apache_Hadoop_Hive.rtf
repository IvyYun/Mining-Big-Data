{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf200
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Bold;\f2\fnil\fcharset0 Menlo-Regular;
\f3\fmodern\fcharset0 Courier;\f4\fmodern\fcharset0 Courier-Bold;\f5\fnil\fcharset0 HelveticaNeue;
\f6\froman\fcharset0 TimesNewRomanPSMT;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red0\green0\blue255;
\red0\green0\blue0;\red255\green0\blue0;\red1\green31\blue103;\red3\green45\blue153;\red0\green66\blue170;
\red18\green145\blue206;\red5\green61\blue204;\red251\green2\blue7;\red255\green255\blue255;\red68\green114\blue196;
\red4\green51\blue255;\red192\green0\blue0;\red0\green86\blue214;\red0\green65\blue141;\red0\green46\blue122;
}
{\*\expandedcolortbl;;\csgray\c0;\csgray\c100000;\cssrgb\c1680\c19835\c100000;
\cssrgb\c0\c0\c0;\csgenericrgb\c100000\c0\c0;\cssrgb\c0\c18185\c48021;\cssrgb\c0\c25950\c66514;\csgenericrgb\c0\c25882\c66667;
\cssrgb\c0\c63852\c84489;\cssrgb\c0\c33896\c83855;\cssrgb\c100000\c14913\c0;\cssrgb\c100000\c100000\c100000;\csgenericrgb\c26667\c44706\c76863;
\csgenericrgb\c1569\c20000\c100000;\csgenericrgb\c75294\c0\c0;\csgenericrgb\c0\c33725\c83922;\csgenericrgb\c0\c25490\c55294;\csgenericrgb\c0\c18039\c47843;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww20220\viewh9440\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Project\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f1\b\fs22 \cf2 \cb3 \CocoaLigature0 Part 1 Data Transformation\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf4 A:\
http://rasinsrv07.cstcis.cti.depaul.edu/CSC555/SSBM1/SSBM_schema_hive.sql\cf2 \
\pard\tx220\tx720\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\li720\fi-720\pardirnatural\qc\partightenfactor0
\ls1\ilvl0\cf4 Hive:
\f2\b0 \cf2 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
ssh -i /Users/ivy/Downloads/ivyyun.pem.txt ec2-user@13.59.83.147\
\
start-dfs.sh; start-yarn.sh; mr-jobhistory-daemon.sh start historyserver\
\
hadoop fs -mkdir -p /user/hive/warehouse\
\
hadoop fs -chmod g+w /tmp\
\
hadoop fs -chmod g+w /user/hive/warehouse\
\
wget http://rasinsrv07.cstcis.cti.depaul.edu/CSC553/data/lineorder.tbl\
\
hadoop fs -put lineorder.tbl /user/ec2-user/data1\
\pard\pardeftab720\ri0\partightenfactor0
\cf0 \cb1 \CocoaLigature1 hadoop fs -ls /user/ec2-user\cf2 \cb3 \CocoaLigature0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0 \cf5 \cb1 \CocoaLigature1 cd $HIVE_HOME\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2 \cf2 \cb3 \CocoaLigature0 rm -rf metastore_db/ 
\f0 \cf6 \cb1 \CocoaLigature1 \
\

\f2 \cf2 \cb3 \CocoaLigature0 $HIVE_HOME/bin/schematool -initSchema -dbType derby\
\
bin/hive\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf4 ###Hive###:\
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf4 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 create table lineorder (\
  lo_orderkey          int,\
  lo_linenumber        int,\
  lo_custkey           int,\
  lo_partkey           int,\
  lo_suppkey           int,\
  lo_orderdate         int,\
  lo_orderpriority     string,\
  lo_shippriority      string,\
  lo_quantity          int,\
  lo_extendedprice     int,\
  lo_ordertotalprice   int,\
  lo_discount          int,\
  lo_revenue           int,\
  lo_supplycost        int,\
  lo_tax               int,\
  lo_commitdate        int,\
  lo_shipmode          string    \
)\
\
ROW FORMAT DELIMITED FIELDS\
TERMINATED BY '|' STORED AS TEXTFILE;\
\
LOAD DATA LOCAL INPATH '/home/ec2-user/lineorder.tbl'\
OVERWRITE INTO TABLE lineorder;\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs22 \cf4 \cb3 \kerning1\expnd0\expndtw0 \CocoaLigature0 time hive -e "INSERT OVERWRITE DIRECTORY '/user/ec2-user/data1/output/' select CONCAT(lo_orderkey,',',lo_linenumber,',',lo_custkey,',',lo_partkey, ',',lo_suppkey,',',lo_orderdate,',',lo_orderpriority,',',lo_shippriority,',',lo_quantity,',',lo_extendedprice,',',lo_ordertotalprice,',',lo_discount,',',lo_revenue,',',lo_supplycost,',',lo_tax,',',lo_commitdate,',',lo_shipmode) from lineorder;"\cf2 \
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf0 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
###Convert to CSV in Hive###\
\pard\pardeftab720\partightenfactor0
\cf7 \cb3 \CocoaLigature0 hadoop fs -cat /user/hive/warehouse/lineorder/lineorder.tbl* > ~/lineorder.csv\
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 \CocoaLigature1 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs22 \cf2 \cb3 \kerning1\expnd0\expndtw0 \CocoaLigature0 hadoop fs -ls /user/hive/warehouse/lineorder\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\fs26 \cf0 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\pardeftab720\qc\partightenfactor0

\f4\b \cf5 ###PIG###\
\pard\pardeftab720\ri0\partightenfactor0

\f2\b0\fs22 \cf0 \kerning1\expnd0\expndtw0 hadoop fs -put lineorder.tbl /user/ec2-user/data
\f3 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\fs26 \cf0 \
\pard\pardeftab720\ri0\partightenfactor0

\f2\fs22 \cf8 \kerning1\expnd0\expndtw0 lineorder1 =LOAD '/user/ec2-user/data' USING PigStorage('|')\
\
AS (lo_orderkey:int,lo_linenumber:int,lo_custkey:int,lo_partkey:int,lo_suppkey:int,lo_orderdate:int,lo_orderpriority:chararray,lo_shippriority:chararray,lo_quantity:int,lo_extendedprice:int,lo_ordertotalprice:float,lo_discount:int,lo_revenue:int,lo_supplycost:int,lo_tax:int,lo_commitdate:int,lo_shipmode:chararray);\
\
store lineorder1 into '/user/ec2-user/lineorder1.csv' using PigStorage(' ');
\fs17 \cf9 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\fs22 \cf8 \cb3 \CocoaLigature0 \
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf0 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\pardeftab720\partightenfactor0
\cf10 ###lineordermapper.py###\
\
#!/usr/bin/python\
import sys\
\
for line in sys.stdin:\
    lineorder = line.strip().replace('|',',')\
    print(lineorder)\
\
###lineorderreducer.py###\
\
#!/usr/bin/python\
import sys\
\
for line in sys.stdin:\
    dates = line.strip().split(',')\
    print(','.join(dates))\
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs22 \cf11 \cb3 \kerning1\expnd0\expndtw0 \CocoaLigature0 time hadoop jar /home/ec2-user/hadoop-2.6.4/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar -input /user/ec2-user/data1/\cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 lineorder.tbl \kerning1\expnd0\expndtw0 -mapper lineordermapper.py -file lineordermapper.py -reducer lineorderreducer.py -file lineorderreducer.py -output /user/ec2-user/data1/lineorder_output_stream\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\fs26 \cf0 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\qc\partightenfactor0

\f4\b \cf12 1B
\f3\b0 \cf5 \
\pard\pardeftab720\qc\partightenfactor0

\f4\b \cf5 ###HIVE###\
\pard\pardeftab720\partightenfactor0

\f3\b0 \cf5 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs22 \cf4 \cb3 \kerning1\expnd0\expndtw0 \CocoaLigature0 time hive -e "INSERT OVERWRITE DIRECTORY '/user/ec2-user/output2_line/' select CONCAT(lo_quantity,' ',lo_extendedprice,' ',lo_ordertotalprice,' ',lo_discount,' ',lo_revenue,' ') from lineorder where lo_discount>=4 and lo_discount<=6;"\
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf0 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\pardeftab720\qc\partightenfactor0

\f4\b \cf0 ###PIG###\
\pard\pardeftab720\partightenfactor0

\f3\b0 \cf0 \
\pard\pardeftab720\ri0\partightenfactor0

\f2\fs22 \cf8 \kerning1\expnd0\expndtw0 lineorder =LOAD '/user/ec2-user/data' USING PigStorage('|')\
\
AS (lo_orderkey:int,lo_linenumber:int,lo_custkey:int,lo_partkey:int,lo_suppkey:int,lo_orderdate:int,lo_orderpriority:chararray,lo_shippriority:chararray,lo_quantity:int,lo_extendedprice:int,lo_ordertotalprice:float,lo_discount:int,lo_revenue:int,lo_supplycost:int,lo_tax:int,lo_commitdate:int,lo_shipmode:chararray);\
\pard\pardeftab720\ri0\partightenfactor0

\f3\fs26 \cf0 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0
\cf7 Filter1 = FILTER lineorder BY (lo_discount>=4 AND lo_discount<=6);\
\
lineordernew =foreach Filter1 generate (lo_quantity, lo_extendedprice, lo_ordertotalprice,lo_discount,lo_revenue);\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs22 \cf7 \cb13 \kerning1\expnd0\expndtw0 \CocoaLigature0 DUMP lineordernew;\expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf7 \cb1 store lineordernew into '
\f2\fs22 \kerning1\expnd0\expndtw0 /user/ec2-user/pigoutput
\f3\fs26 \expnd0\expndtw0\kerning0
\'92 using PigStorage(' ');\cf0 \
\
\
\
\pard\pardeftab720\qc\partightenfactor0

\f4\b \cf12 1C\
\pard\pardeftab720\partightenfactor0
\cf4 ###HIVE###\
\pard\pardeftab720\partightenfactor0
\cf12 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\b0\fs22 \cf2 \cb3 \kerning1\expnd0\expndtw0 \CocoaLigature0 hive> create table dwdate (\
    > d_datekey            int,\
    > d_date               string,\
    > d_dayofweek          string,\
    > d_month              string,\
    > d_year               int,\
    > d_yearmonthnum       int,\
    > d_yearmonth          string,\
    > d_daynuminweek       int,\
    > d_daynuminmonth      int,\
    > d_daynuminyear       int,\
    > d_monthnuminyear     int,\
    > d_weeknuminyear      int,\
    > d_sellingseason      string,\
    > d_lastdayinweekfl    string,\
    > d_lastdayinmonthfl   string,\
    > d_holidayfl          string,\
    > d_weekdayfl          string\
    > )\
    > ROW FORMAT DELIMITED FIELDS\
    > TERMINATED BY '|' STORED AS TEXTFILE;\
\
hive> LOAD DATA LOCAL INPATH '/home/ec2-user/dwdate.tbl'\
    > OVERWRITE INTO TABLE dwdate;
\f3\fs26 \cf0 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\pardeftab720\partightenfactor0
\cf0 \
time hive -e "INSERT OVERWRITE DIRECTORY '/user/ec2-user/output3/\'92 SELECT CONCAT(lo_partkey,'|', lo_suppkey,'|', lo_discount,'|', d_year,'|', lo_revenue) FROM lineorder, dwdate WHERE lo_orderdate = d_datekey;"\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf4 ###PIG###\
\pard\pardeftab720\partightenfactor0

\f3\b0 \cf5 \
\pard\pardeftab720\partightenfactor0
\cf8 lineorder =LOAD '/user/ec2-user/data1/lineorder.tbl' USING PigStorage('|') AS (lo_orderkey:int,lo_linenumber:int,lo_custkey:int,lo_partkey:int,lo_suppkey:int,lo_orderdate:int,lo_orderpriority:chararray,lo_shippriority:chararray,lo_quantity:int,lo_extendedprice:int,lo_ordertotalprice:float,lo_discount:int,lo_revenue:float,lo_supplycost:float,lo_tax:float,lo_commitdate:int,lo_shipmode:chararray);\
\
dwdate =LOAD '/user/ec2-user/data1/dwdate.tbl' USING PigStorage('|') AS (d_datekey:int,d_date:chararray,d_dayofweek:chararray,d_month:chararray,d_year:int,d_yearmonthnum:int,d_yearmonth:chararray,d_daynuminweek:int,d_daynuminmonth:int,d_daynuminyear:int,d_monthnuminyear:int,d_weeknuminyear:int,d_sellingseason:chararray,d_lastdayinweekfl:chararray,d_lastdayinmonthfl:chararray,d_holidayfl:chararray,d_weekdayfl:chararray);\
\
joinnew =JOIN lineorder BY lo_orderdate, dwdate BY d_datekey;\
\
pignew =foreach joinnew generate (lo_partkey, lo_suppkey, lo_discount, d_year, lo_revenue);\
\
\pard\pardeftab720\ri0\partightenfactor0

\f2\fs22 \cf7 \kerning1\expnd0\expndtw0 store pignew into '/user/ec2-user/data1/pigoutput2' using PigStorage('|');\
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf8 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf5 PART 2\
\pard\pardeftab720\partightenfactor0
\cf4 2 A
\f3\b0 \cf5 \
\pard\pardeftab720\partightenfactor0

\f4\b \cf5 ###HIVE###\
\pard\pardeftab720\partightenfactor0

\f3\b0 \cf8 create table part (\
p_partkey     int,\
p_name        string,\
p_mfgr        string,\
p_category    string,\
p_brand1      string,\
p_color       string,\
p_type        string,\
p_size        int,\
p_container   string\
)\
ROW FORMAT DELIMITED FIELDS\
TERMINATED BY '|' STORED AS TEXTFILE;\
\
LOAD DATA LOCAL INPATH '/home/ec2-user/part.tbl'\
OVERWRITE INTO TABLE part;\
\
create table supplier (\
s_suppkey     int,\
s_name        string,\
s_address     string,\
s_city        string,\
s_nation      string,\
s_region      string,\
s_phone       string\
)\
ROW FORMAT DELIMITED FIELDS\
TERMINATED BY '|' STORED AS TEXTFILE;\
\
LOAD DATA LOCAL INPATH '/home/ec2-user/supplier.tbl'\
OVERWRITE INTO TABLE supplier;\
\
create table customer (\
c_custkey     int,\
c_name        string,\
c_address     string,\
c_city        string,\
c_nation      string,\
c_region      string,\
c_phone       string,\
c_mktsegment  string\
)\
ROW FORMAT DELIMITED FIELDS\
TERMINATED BY '|' STORED AS TEXTFILE;\
\
LOAD DATA LOCAL INPATH '/home/ec2-user/customer.tbl'\
OVERWRITE INTO TABLE customer;\cf0 \
\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf12 2.2
\f3\b0 \
\pard\pardeftab720\partightenfactor0
\cf0 time hive -e "select sum(lo_revenue), d_year, p_brand1 from lineorder, dwdate, part, supplier where lo_orderdate = d_datekey and lo_partkey = p_partkey\
and lo_suppkey = s_suppkey and p_brand1 between 'MFGR#2221' and 'MFGR#2228'\
and s_region = 'ASIA' group by d_year, p_brand1 order by d_year, p_brand1;"\cf5 \
\cf0 \
\pard\pardeftab720\partightenfactor0

\f4\b \cf12 3.2
\f3\b0 \cf5 \
\pard\pardeftab720\partightenfactor0
\cf0 time hive -e "select c_city, s_city, d_year, sum(lo_revenue) as revenue from customer, lineorder, supplier, dwdate where lo_custkey = c_custkey and lo_suppkey = s_suppkey and lo_orderdate = d_datekey and c_nation = 'UNITED STATES' and s_nation = 'UNITED STATES' and d_year between 1992 and 1997 and d_year >= 1992 and d_year <= 1997 group by c_city, s_city, d_year order by d_year asc, revenue asc;"\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf12 4.2
\f3\b0 \cf5 \
\pard\pardeftab720\partightenfactor0
\cf0 time hive -e "select d_year, s_nation, p_category, sum(lo_revenue) as profit1 from lineorder, customer, supplier, part, dwdate where lo_custkey = c_custkey and lo_suppkey = s_suppkey and lo_partkey = p_partkey and lo_orderdate = d_datekey and c_region = 'AMERICA' and s_region = 'AMERICA' and d_year = 1997 and p_mfgr = 'MFGR#1' group by d_year, s_nation, p_category;"\
\
\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf12 2B
\f3\b0 \cf5 \
\pard\pardeftab720\partightenfactor0

\f4\b \cf5 ###HIVE###\
\pard\pardeftab720\partightenfactor0

\f3\b0 \cf7 create table prejoin1 (\
lo_partkey int,\
lo_suppkey int,\
lo_discount int,\
d_year int,\
lo_revenue int\
)\
ROW FORMAT DELIMITED FIELDS\
TERMINATED BY '|' STORED AS TEXTFILE;\
\
LOAD DATA LOCAL INPATH '/home/ec2-user/output3/\'91\
OVERWRITE INTO TABLE prejoin1;\
\
\
\
time hive -e "select sum(lo_revenue), d_year, p_brand1\
from prejoin1, part, supplier\
where lo_partkey = p_partkey\
and lo_suppkey = s_suppkey\
and p_category = 'MFGR#12'\
and s_region = 'AMERICA'\
group by d_year, p_brand1\
order by d_year, p_brand1;"\cf5 \
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf5 ###PIG###
\f3\b0 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f2\fs22 \cf7 \cb3 \kerning1\expnd0\expndtw0 \CocoaLigature0 prejoin =LOAD '/user/ec2-user/data1/
\f1\b pigoutput2
\f2\b0 ' USING PigStorage('|') AS (lo_partkey:int,lo_suppkey:int,lo_discount:chararray,d_year:int,lo_revenue:int);\
\
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf7 \cb1 \expnd0\expndtw0\kerning0
\CocoaLigature1 part =LOAD '/user/ec2-user/data1/part.tbl' USING PigStorage('|') AS (p_partkey:int,p_name:chararray,p_mfgr:chararray,p_category:chararray,p_brand1:chararray,p_color:chararray,p_type:chararray,p_size:int,p_container:chararray);\
\
supplier =LOAD '/user/ec2-user/data1/supplier.tbl' USING PigStorage('|') AS (s_suppkey:int,s_name:chararray,s_address:chararray,s_city:chararray,s_nation:chararray,s_region:chararray,s_phone:chararray);\
\
customer =LOAD '/user/ec2-user/data1/customer.tbl' USING PigStorage('|') AS (c_custkey:int,c_name:chararray,c_address:chararray,c_city:chararray,c_nation:chararray,c_region:chararray,c_phone:chararray,c_mktsegment:chararray);\
\
filter1 =filter part by p_category MATCHES 'MFGR#12';\
\
filter2 =filter supplier by s_region MATCHES 'AMERICA';\
\
join1 =join prejoin by lo_partkey, filter1 by p_partkey;\
\
join2 =join join1 by prejoin::lo_suppkey, filter2 BY s_suppkey;\
\
group1 =group join2 by (join1::prejoin::d_year,join1::filter1::p_brand1);\
\
final =FOREACH group1 GENERATE SUM(join2.join1::prejoin::lo_revenue),FLATTEN(group) as (year, brand1);\
\
order1 =ORDER final BY year, brand1;\
\
DUMP order1;\
\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf12 Part 3\
A
\f3\b0 \
\
\pard\pardeftab720\partightenfactor0

\f2\fs22 \cf8 \kerning1\expnd0\expndtw0 time mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job\
\
mahout clusterdump --input output/clusters-10-final --pointsDir output/clusteredPoints --output clusteranalyze.txt\
\pard\pardeftab720\partightenfactor0

\f0\fs20 \cf8 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\b \cf12 B
\b0 \cf5 \
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf0 \expnd0\expndtw0\kerning0
#!/usr/bin/python\
import random\
\
for i in range(7):\
	print "%i %i %i %i %i %i" %(
\f5 \kerning1\expnd0\expndtw0 i,random.randint(0,30),random.randint(90000,5000000),random.randint(90000,5000000),random.randint(3,5),random.randint(90000,6000000))\
\pard\pardeftab720\partightenfactor0

\f0\fs20 \cf0 \
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf0 \expnd0\expndtw0\kerning0
python centroids.py > centroids.txt
\f0\fs20 \kerning1\expnd0\expndtw0 \
\
\
\pard\tx0\tx0\pardeftab720\partightenfactor0

\f1\b\fs24 \cf0 ###clusteringmapper.py###\
\pard\tx0\tx0\pardeftab720\partightenfactor0

\f3\b0\fs26 \cf0 \expnd0\expndtw0\kerning0
#!/usr/bin/python\
import sys\
\pard\tx0\tx0\pardeftab720\partightenfactor0

\f0\fs20 \cf0 \kerning1\expnd0\expndtw0 \
\pard\tx0\tx0\pardeftab720\partightenfactor0

\f3\fs26 \cf0 \expnd0\expndtw0\kerning0
file=open('centroids.txt','r')\
list=[]\
\
def dist(x,y):\
	sum=0\
	for i in range(0,5):\
		sum=sum+(float(x[i])-float(y[i+1]))**2\
		return total**0.5\
\
for line in file:\
	list.append(line.strip().split(' '))\
\
for line in sys.stdin:\
	line=line.strip()\
	item=line.split(' ')\
	if len(item)>0:\
		distance=[]\
	for i in range(0,7):\
		distance.append(dist(item,list[i]))\
		key=distance.index(min(distance))\
		print "%i\\t%s" % (key,line)
\f0\fs20 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0

\f2\fs22 \cf7 cp /home/ec2-user/hadoop-2.6.4/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar /home/ec2-user/hadoop-2.6.4/
\f0\fs24 \cf14 \
\pard\pardeftab720\partightenfactor0

\fs20 \cf0 \
\pard\pardeftab720\partightenfactor0

\f2\fs22 \cf7 \expnd0\expndtw0\kerning0
time hadoop jar /home/ec2-user/hadoop-2.6.4/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar -input /user/ec2-user/\kerning1\expnd0\expndtw0 data1/output2_line/\expnd0\expndtw0\kerning0
 -mapper clusteringmapper.py -file centroids.txt -file clusteringmapper.py -reducer clusteringreducer.py -file clusteringreducer.py -output clustering_output\cf0 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf7 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf12 Part4\
A
\f3\b0 \cf7 \
\pard\pardeftab720\partightenfactor0

\f4\b \cf7 ###HIVE###
\f3\b0 \
\pard\pardeftab720\ri0\partightenfactor0

\f2\fs22 \cf15 \kerning1\expnd0\expndtw0 time hive -e "INSERT OVERWRITE DIRECTORY '/user/ec2-user/data1/output/' select CONCAT(lo_orderkey,',',lo_linenumber,',',lo_custkey,',',lo_partkey, ',',lo_suppkey,',',lo_orderdate,',',lo_orderpriority,',',lo_shippriority,',',lo_quantity,',',lo_extendedprice,',',lo_ordertotalprice,',',lo_discount,',',lo_revenue,',',lo_supplycost,',',lo_tax,',',lo_commitdate,',',lo_shipmode) from lineorder;"
\f6\fs17 \cf0 \
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf7 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\ri0\partightenfactor0

\f2\fs22 \cf16 \kerning1\expnd0\expndtw0 A cluster with 4 nodes: (scale4)\
Hive:\
\pard\pardeftab720\ri0\partightenfactor0
\cf17 time hadoop jar /home/ec2-user/hadoop-2.6.4/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar -input /user/ec2-user/data1/lineorder.tbl -mapper lineordermapper.py -file lineordermapper.py -reducer lineorderreducer.py -file lineorderreducer.py -output /user/ec2-user/data1/lineorder_output_stream
\fs17 \
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f3\fs26 \cf7 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf7 ###PIG###
\f3\b0 \
\pard\pardeftab720\ri0\partightenfactor0

\f2\fs22 \cf0 \kerning1\expnd0\expndtw0 lineorder =LOAD '/user/ec2-user/data1' USING PigStorage('|')
\f6 \
\
\pard\pardeftab720\ri0\partightenfactor0

\f2 \cf9 AS (lo_orderkey:int,lo_linenumber:int,lo_custkey:int,lo_partkey:int,lo_suppkey:int,lo_orderdate:int,lo_orderpriority:chararray,lo_shippriority:chararray,lo_quantity:int,lo_extendedprice:int,lo_ordertotalprice:float,lo_discount:int,lo_revenue:int,lo_supplycost:int,lo_tax:int,lo_commitdate:int,lo_shipmode:chararray);\
\pard\pardeftab720\ri0\partightenfactor0

\f3 \cf0 \
\pard\pardeftab720\ri0\partightenfactor0

\f2 \cf9 store lineorder into '/user/ec2-user/data1/lineorder2.csv' using PigStorage(' ');\
\pard\pardeftab720\partightenfactor0

\f3\fs26 \cf7 \expnd0\expndtw0\kerning0
\
\
\pard\pardeftab720\partightenfactor0

\f4\b \cf7 B\
\pard\pardeftab720\partightenfactor0

\f3\b0 \cf7 \
\pard\pardeftab720\partightenfactor0

\f4\b \cf7 ###PIG###\
\pard\pardeftab720\ri0\partightenfactor0

\f2\b0\fs22 \cf18 \kerning1\expnd0\expndtw0 prejoin1 =LOAD '/user/ec2-user/data1/\cf19 pigoutput2\cf18 ' USING PigStorage('|') \
AS (lo_partkey:int,lo_suppkey:int,lo_discount:chararray,d_year:int,lo_revenue:int);\
\
part =LOAD '/user/ec2-user/data1/part.tbl' USING PigStorage('|') \
AS (p_partkey:int,p_name:chararray,p_mfgr:chararray,p_category:chararray,p_brand1:chararray,p_color:chararray,p_type:chararray,p_size:int,p_container:chararray);\
\
supplier =LOAD '/user/ec2-user/data1/supplier.tbl' USING PigStorage('|') \
AS (s_suppkey:int,s_name:chararray,s_address:chararray,s_city:chararray,s_nation:chararray,s_region:chararray,s_phone:chararray);\
\
customer =LOAD '/user/ec2-user/data1/customer.tbl' USING PigStorage('|') \
AS (c_custkey:int,c_name:chararray,c_address:chararray,c_city:chararray,c_nation:chararray,c_region:chararray,c_phone:chararray,c_mktsegment:chararray);\
\
filter1 =filter part by p_category MATCHES 'MFGR#12';\
filter2 =filter supplier by s_region MATCHES 'AMERICA';\
join1 =join prejoin1 by lo_partkey, filter1 by p_partkey;\
join2 =join join1 by prejoin1::lo_suppkey, filter2 BY s_suppkey;\
group1 =group join2 by (join1::prejoin1::d_year,join1::filter1::p_brand1);\
\
final =FOREACH group1 GENERATE SUM(join2.join1::prejoin1::lo_revenue),FLATTEN(group) as (year, brand1);\
order1 =ORDER final BY year, brand1;\
\
DUMP order1;
\fs17 \
\

\f1\b\fs24 \cf0 Scale14: a cluster of at least 4 nodes\
\
\pard\pardeftab720\ri0\partightenfactor0

\f2\b0\fs22 \cf14 time hadoop jar /home/ec2-user/hadoop-2.6.4/share/hadoop/tools/lib/hadoop-streaming-2.6.4.jar -input /user/ec2-user/data1/lineorder.tbl.1 -mapper lineordermapper.py -file lineordermapper.py -reducer lineorderreducer.py -file lineorderreducer.py -output /user/ec2-user/data1/lineorder_output_scale14
\f6\fs24 \
\pard\pardeftab720\ri0\partightenfactor0

\f3\fs20 \cf19 \
\pard\pardeftab720\partightenfactor0

\fs26 \cf7 \expnd0\expndtw0\kerning0
\
}
